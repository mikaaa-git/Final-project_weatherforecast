# -*- coding: utf-8 -*-
"""Bigdata PM2.5

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18OISYEDpd2Oq7p2e-WXCuHHp-p1G6rma
"""

!pip install pyspark

"""# **Forest**"""

# Install PySpark
!pip install pyspark

# Import libraries from PySpark
import pandas as pd
from pyspark.sql import SparkSession
from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder, StandardScaler
from pyspark.ml.regression import RandomForestRegressor
from pyspark.ml import Pipeline
from pyspark.sql.functions import col, to_timestamp, hour, dayofweek, month, dayofmonth, avg
from pyspark.sql.window import Window
from google.colab import files
import matplotlib.pyplot as plt

# Create a SparkSession
spark = SparkSession.builder.appName("PM2.5 Prediction Phra Nakhon").getOrCreate()

# Load the data
data = pd.read_csv('weather_data.csv')  # Assuming your file name is 'weather_data.csv'

# Convert to PySpark DataFrame
df = spark.createDataFrame(data)

# Data Preprocessing
# Convert 'timestamp' column to datetime
df = df.withColumn('timestamp', to_timestamp('timestamp'))

# Filter data for the selected district (Phra Nakhon)
df = df.filter(df['district'] == 'Phra Nakhon')

# Extract time-related features like Hour, Day of the Week, Month, and Day of the Month
df = df.withColumn('Hour', hour('timestamp')) \
       .withColumn('DayOfWeek', dayofweek('timestamp')) \
       .withColumn('Month', month('timestamp')) \
       .withColumn('Day', dayofmonth('timestamp'))

# Handle missing values
df = df.na.fill({'PM2_5': df.select('PM2_5').dropna().agg({'PM2_5': 'mean'}).collect()[0][0]})

# Create time windows (24 hours, 3 days, 7 days)
window_spec_24h = Window.orderBy(col("timestamp").cast("long")).rangeBetween(-24 * 60 * 60, 0)  # 24 hours = 24 * 60 * 60 seconds
window_spec_3d = Window.orderBy(col("timestamp").cast("long")).rangeBetween(-72 * 60 * 60, 0)  # 3 Days = 72 hours = 72 * 60 * 60 seconds
window_spec_7d = Window.orderBy(col("timestamp").cast("long")).rangeBetween(-7 * 24 * 60 * 60, 0)  # 7 Days = 7 * 24 * 60 * 60 seconds

# Aggregating features for 24 hours, 3 days, and 7 days
df = df.withColumn("avg_temp_24h", avg("temp").over(window_spec_24h))
df = df.withColumn("avg_humidity_24h", avg("humidity").over(window_spec_24h))
df = df.withColumn("avg_PM2_5_24h", avg("PM2_5").over(window_spec_24h))

df = df.withColumn("avg_temp_3d", avg("temp").over(window_spec_3d))
df = df.withColumn("avg_humidity_3d", avg("humidity").over(window_spec_3d))
df = df.withColumn("avg_PM2_5_3d", avg("PM2_5").over(window_spec_3d))

df = df.withColumn("avg_temp_7d", avg("temp").over(window_spec_7d))
df = df.withColumn("avg_humidity_7d", avg("humidity").over(window_spec_7d))
df = df.withColumn("avg_PM2_5_7d", avg("PM2_5").over(window_spec_7d))

# Feature Selection
# for training the model
feature_columns = ['Hour', 'DayOfWeek', 'Month', 'Day', 'lat', 'lon', 'avg_temp_24h', 'avg_humidity_24h', 'avg_PM2_5_24h',
                   'avg_temp_3d', 'avg_humidity_3d', 'avg_PM2_5_3d', 'avg_temp_7d', 'avg_humidity_7d', 'avg_PM2_5_7d']

# Features into a Single Vector Column
assembler = VectorAssembler(inputCols=feature_columns, outputCol="features")

# Split Data into Training and Test Sets
train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)

# Scale the Features (optional but recommended for some models)
scaler = StandardScaler(inputCol="features", outputCol="scaled_features")
train_data = assembler.transform(train_data)
train_data = scaler.fit(train_data).transform(train_data)
test_data = assembler.transform(test_data)
test_data = scaler.fit(test_data).transform(test_data)

# Train a Random Forest Regressor Model
rf = RandomForestRegressor(featuresCol="scaled_features", labelCol="PM2_5", numTrees=50, maxDepth=10, seed=42)

# Train the model
rf_model = rf.fit(train_data)

# Make Predictions
predictions = rf_model.transform(test_data)

# Evaluate the Model
from pyspark.ml.evaluation import RegressionEvaluator

# Evaluate using RMSE (Root Mean Squared Error)
evaluator = RegressionEvaluator(labelCol="PM2_5", predictionCol="prediction", metricName="rmse")
rmse = evaluator.evaluate(predictions)


# Evaluate using R-squared
evaluator_r2 = RegressionEvaluator(labelCol="PM2_5", predictionCol="prediction", metricName="r2")
r2 = evaluator_r2.evaluate(predictions)


predictions_df = predictions.select("PM2_5", "prediction").toPandas()


# Filter the data for the last 24 hours, 3 days, and 7 days
df_24h = df.filter(df['timestamp'] >= df.select("timestamp").orderBy("timestamp", ascending=False).limit(1).collect()[0][0] - pd.Timedelta(hours=24))
df_3d = df.filter(df['timestamp'] >= df.select("timestamp").orderBy("timestamp", ascending=False).limit(1).collect()[0][0] - pd.Timedelta(days=3))
df_7d = df.filter(df['timestamp'] >= df.select("timestamp").orderBy("timestamp", ascending=False).limit(1).collect()[0][0] - pd.Timedelta(days=7))

# Save the plots as separate images

# Plot for the last 24 hours
plt.figure(figsize=(10, 6))
plt.plot(df_24h.select("timestamp").toPandas(), df_24h.select("PM2_5").toPandas(), label="PM2.5", color='blue')
plt.title('PM2.5 Levels (Last 24 hours)')
plt.xlabel('Time')
plt.ylabel('PM2.5')
plt.grid(True)
plt.tight_layout()
plt.savefig('/content/PM2_5_24h_rf.png')  # Save as image
print("Saved PM2.5 Levels (Last 24 hours) graph to PM2_5_24h_rf.png")

# Plot for the last 3 days
plt.figure(figsize=(10, 6))
plt.plot(df_3d.select("timestamp").toPandas(), df_3d.select("PM2_5").toPandas(), label="PM2.5", color='orange')
plt.title('PM2.5 Levels (Last 3 days)')
plt.xlabel('Time')
plt.ylabel('PM2.5')
plt.grid(True)
plt.tight_layout()
plt.savefig('/content/PM2_5_3d_rf.png')  # Save as image
print("Saved PM2.5 Levels (Last 3 days) graph to PM2_5_3d_rf.png")

# Plot for the last 7 days
plt.figure(figsize=(10, 6))
plt.plot(df_7d.select("timestamp").toPandas(), df_7d.select("PM2_5").toPandas(), label="PM2.5", color='green')
plt.title('PM2.5 Levels (Last 7 days)')
plt.xlabel('Time')
plt.ylabel('PM2.5')
plt.grid(True)
plt.tight_layout()
plt.savefig('/content/PM2_5_7d_rf.png')  # Save as image
print("Saved PM2.5 Levels (Last 7 days) graph to PM2_5_7d_rf.png")

# Show some of the predictions vs actual values
predictions_df = predictions.select("timestamp", "PM2_5", "prediction").toPandas()

# Plot the actual vs predicted values for the entire dataset (chosen district)

plt.figure(figsize=(10, 6))

# Plot Actual vs Predicted PM2.5 levels
plt.plot(predictions_df['timestamp'], predictions_df['PM2_5'], label="Actual PM2.5", color='blue')
plt.plot(predictions_df['timestamp'], predictions_df['prediction'], label="Predicted PM2.5", color='red', linestyle='--')

plt.title('PM2.5 Levels: Actual vs Predicted (Phra Nakhon)')
plt.xlabel('Time')
plt.ylabel('PM2.5')
plt.legend()
plt.grid(True)
plt.tight_layout()

# Save the plot as an image
plt.savefig('/content/PM2_5_actual_vs_predicted.png')  # Save as image
print("Saved PM2.5 Levels (Actual vs Predicted) graph to PM2_5_actual_vs_predicted.png")

# Download the image
files.download('/content/PM2_5_actual_vs_predicted.png')

"""# **Forest future**"""

from google.colab import files
import matplotlib.pyplot as plt

# Split Data into Training and Test Sets
train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)

# Scale the Features
scaler = StandardScaler(inputCol="features", outputCol="scaled_features")
# Fit the scaler on the training data and transform both train and test

# Create the 'features' column before scaling
train_data = assembler.transform(train_data)
test_data = assembler.transform(test_data)

scaler_model = scaler.fit(train_data)  # Fit on training data with 'features' column present
train_data = scaler_model.transform(train_data)  # Transform training data
test_data = scaler_model.transform(test_data)  # Transform test data

# Check the range of PM2.5 in the training data
df.select("PM2_5").describe().show()

# Train a Random Forest Regressor Model
rf = RandomForestRegressor(featuresCol="scaled_features", labelCol="PM2_5")
rf_model = rf.fit(train_data)

# Make Predictions with the Random Forest Regression Model
rf_predictions = rf_model.transform(test_data)

# Evaluate the Model Using RMSE and R2
evaluator = RegressionEvaluator(labelCol="PM2_5", predictionCol="prediction", metricName="rmse")
rf_rmse = evaluator.evaluate(rf_predictions)

evaluator_r2 = RegressionEvaluator(labelCol="PM2_5", predictionCol="prediction", metricName="r2")
rf_r2 = evaluator_r2.evaluate(rf_predictions)

print(f"Random Forest RMSE: {rf_rmse}")
print(f"Random Forest R2: {rf_r2}")

# Make Future Predictions for Next 3 Days
# Prepare future data (example: create future feature set for 3 days ahead)
future_data = pd.DataFrame({
    'Day': [21, 22, 23],
    'Month': [3, 3, 3],
    'Hour': [12, 12, 12],
    'lat': [13.75, 13.75, 13.75],  # Example coordinates (change if needed)
    'lon': [100.50, 100.50, 100.50],  # Example coordinates (change if needed)
    'temp': [30, 31, 32],
    'feels_like': [32, 33, 34],
    'pressure': [1013, 1014, 1015],
    'humidity': [70, 68, 65],
    'clouds': [10, 20, 15],
    'wind_speed': [5, 5.5, 6],
    'wind_deg': [200, 210, 220]
})

# Convert future data to PySpark DataFrame
future_df = spark.createDataFrame(future_data)

# Assemble features for future predictions
future_df = assembler.transform(future_df)

# Transform using the same fitted scaler
future_df = scaler_model.transform(future_df)

# Make predictions using the Random Forest Model
rf_future_predictions = rf_model.transform(future_df)

# Step 19: Get predictions and classify as High or Not High (based on a threshold)
threshold = 50  # Example threshold for High PM2.5

rf_future_predictions_df = rf_future_predictions.select("Day", "Month", "Hour", "prediction").toPandas()

# Classify as High or Not High based on threshold
rf_future_predictions_df['PM2.5_Status'] = rf_future_predictions_df['prediction'].apply(lambda x: "HighðŸ”´" if x > threshold else "Not High")

# Display results for the Random Forest Model
print("\nRandom Forest Predictions for the Next 3 Days:")
print(rf_future_predictions_df)

