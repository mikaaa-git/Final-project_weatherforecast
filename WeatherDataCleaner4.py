# -*- coding: utf-8 -*-
"""weather4DataCleaning3OurlierNullDup.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SQZKIW_1eR0MhiKu_3tCJnzuMuHgfcA4
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, mean, stddev, count, when, udf
from pyspark.sql.types import StringType, IntegerType
import matplotlib.pyplot as plt
from datetime import datetime
from functools import reduce
from pyspark.sql import functions as F

# Initialize Spark session
spark = SparkSession.builder.appName("DataCleaning").getOrCreate()

# Load CSV file
data_path = "weather_data.csv"
df = spark.read.csv(data_path, header=True, inferSchema=True)

# Define the format_timestamp function
def format_timestamp(timestamp):
    return datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d %H:%M:%S')

format_timestamp_udf = udf(format_timestamp, StringType())
df = df.withColumn('FormattedDateTime', format_timestamp_udf(col('timestamp').cast(IntegerType())))

# Initial counts
initial_count = df.count()
initial_duplicates_df = df.exceptAll(df.dropDuplicates())
initial_duplicates = initial_duplicates_df.count()
initial_nulls = df.select([count(when(col(c).isNull(), 1)).alias(c) for c in df.columns]).collect()

# Print duplicate rows
if initial_duplicates > 0:
    print("Duplicate rows found:")
    initial_duplicates_df.show()

# Remove duplicates
df = df.dropDuplicates()
print(f"Row count after removing duplicates: {df.count()}")

# Print rows that had all null values before removal
null_rows_df = df.filter(
    reduce(lambda acc, c: acc + (when(col(c).isNull(), 1).otherwise(0)), df.columns, F.lit(0)) == len(df.columns)
)

if null_rows_df.count() > 0:
    print("Rows with all null values:")
    null_rows_df.show()

# Drop rows with missing values
df = df.dropna(how='all')

filtered_numeric_columns = [col_name for col_name, dtype in df.dtypes if dtype in ('int', 'double', 'float') and col_name not in ["lat", "lon","wind_deg"]]

# Detect and remove outliers using IQR method
outlier_counts = {}  # Store outlier counts for each column

def remove_outliers_iqr(df, column):
    # Calculate Q1 (25th percentile) and Q3 (75th percentile)
    quantiles = df.approxQuantile(column, [0.25, 0.75], 0.05)  # Approximate with a relative error of 5%
    q1, q3 = quantiles[0], quantiles[1]
    iqr = q3 - q1  # Interquartile range

    # Calculate bounds for outlier detection
    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr

    # Filter out rows that are outside the bounds
    outliers_df = df.filter((col(column) < lower_bound) | (col(column) > upper_bound))
    outlier_counts[column] = outliers_df.count()

    return df.filter((col(column) >= lower_bound) & (col(column) <= upper_bound))

# Apply the IQR method for outlier detection
for col_name in filtered_numeric_columns:
    df = remove_outliers_iqr(df, col_name)
    print(f"Row count after removing outliers from {col_name} using IQR: {df.count()}")

# Generate scatter plots for each column with outliers, ordered by district
for district in df.select("District").distinct().rdd.flatMap(lambda x: x).collect():
    district_df = df.filter(col("District") == district)
    for col_name in filtered_numeric_columns:
        if outlier_counts.get(col_name, 0) > 0:
            # Collect data for scatter plot
            x_values = district_df.select(col_name).rdd.flatMap(lambda x: x).collect()
            y_values = list(range(len(x_values)))

            # Calculate IQR bounds for outlier detection (using the same logic as before)
            quantiles = district_df.approxQuantile(col_name, [0.25, 0.75], 0.05)
            q1, q3 = quantiles[0], quantiles[1]
            iqr = q3 - q1
            lower_bound = q1 - 1.5 * iqr
            upper_bound = q3 + 1.5 * iqr

            # Identify outlier indices
            outlier_indices = [i for i, val in enumerate(x_values) if val < lower_bound or val > upper_bound]

            # Create scatter plot with outliers highlighted
            plt.figure(figsize=(10, 5))
            plt.scatter(y_values, x_values, label='Data', color='blue', alpha=0.5)  # Regular data points
            plt.scatter([y_values[i] for i in outlier_indices], [x_values[i] for i in outlier_indices],
                        label='Outliers', color='red', marker='o', s=50)  # Outliers in red
            plt.xlabel("Index")
            plt.ylabel(col_name)
            plt.title(f"Scatter Plot of {col_name} in {district} (IQR Outliers)")
            plt.legend()
            plt.show()

# Final counts
final_count = df.count()
final_nulls = df.select([count(when(col(c).isNull(), 1)).alias(c) for c in df.columns]).collect()

# Order by District before saving
df = df.orderBy("District", "FormattedDateTime")

# Save cleaned data to CSV
output_path = "cleaned_weather_data.csv"
df.write.csv(output_path, header=True, mode='overwrite')

# Print report
print("Data Cleaning Report:")
print(f"Initial row count: {initial_count}")
print(f"Duplicate rows removed: {initial_duplicates}")
print("Initial Null Values:")
print(initial_nulls)
print(f"Final row count: {final_count}")
print("Final Null Values:")
print(final_nulls)
print("Data cleaning complete. Saved to", output_path)

# Stop Spark session
spark.stop()